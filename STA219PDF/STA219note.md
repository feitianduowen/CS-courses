STA219 note

# 0 ç§¯åˆ†å…¬å¼

1. 

$$
\int_{a}^{b} f(x)\, g'(x)\,dx
   = \bigl[f(x)g(x)\bigr]_{a}^{b}
- \int_{a}^{b} f'(x)\, g(x)\,dx
$$

2. $$
   J_F(x,y)=
   \begin{bmatrix}
   \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y}\\
   \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y}
   \end{bmatrix}.
   $$
3. $d\tan x=\sec^2x$
   
   $d\sec x=\sec x\tan x$
   
   $d\cot x=-\csc^2x$
   
   $d\csc x=-\csc x\cot x$
   
   $d\sin^{-1}x=\frac{1}{\sqrt{1-x^2}}$
   
   $d\cos^{-1}x=-\frac{1}{\sqrt{1-x^2}}$
   
   $d\tan^{-1}x=\frac{1}{1+x^2}$
4. Taylor series
   
   $$
   \frac{1}{1-x}
    = 1 + x + x^2 + \cdots + x^n + \cdots
    = \sum_{n=0}^{\infty} x^n,\qquad |x|<1.
   $$
   
   $$
   \frac{1}{1+x}
    = 1 - x + x^2 - \cdots + (-x)^n + \cdots
    = \sum_{n=0}^{\infty} (-1)^n x^n,\qquad |x|<1.
   $$
   
   $$
   e^{x}
    = 1 + x + \frac{x^{2}}{2!} + \cdots + \frac{x^{n}}{n!} + \cdots
    = \sum_{n=0}^{\infty} \frac{x^{n}}{n!},\qquad x\in\mathbb{R}.
   $$
   
   $$
   \sin x
    = x - \frac{x^{3}}{3!} + \frac{x^{5}}{5!} - \cdots + (-1)^n\frac{x^{2n+1}}{(2n+1)!} + \cdots
    = \sum_{n=0}^{\infty} (-1)^n \frac{x^{2n+1}}{(2n+1)!},\qquad x\in\mathbb{R}.
   $$
   
   $$
   \cos x
    = 1 - \frac{x^{2}}{2!} + \frac{x^{4}}{4!} - \cdots + (-1)^n\frac{x^{2n}}{(2n)!} + \cdots
    = \sum_{n=0}^{\infty} (-1)^n \frac{x^{2n}}{(2n)!},\qquad x\in\mathbb{R}.
   $$
   
   $$
   \ln(1+x)
    = x - \frac{x^{2}}{2} + \frac{x^{3}}{3} - \cdots + (-1)^{n-1}\frac{x^{n}}{n} + \cdots
    = \sum_{n=1}^{\infty} (-1)^{n-1}\frac{x^{n}}{n},\qquad -1<x\le 1.
   $$
   
   $$
   \tan^{-1}x
    = x-\frac{x^{3}}{3}+\frac{x^{5}}{5}-\cdots+(-1)^n\frac{x^{2n+1}}{2n+1}+\cdots
    = \sum_{n=0}^{\infty}(-1)^n\frac{x^{2n+1}}{2n+1},\qquad |x|\le 1.
   $$

# 1 Basic Ideas in Probability

1. **å®¹æ–¥åŸç†**:
   $P(A_1\cap A_2\cap...\cap A_n)=\sum_{i=1}^{n}P(A_i)-\sum_{1\leq i<j\leq n}P(A_iA_j)+...(-1)^{n-1}P(A_1A_2...A_n)$
2. $C_n^k=(_k^n)=\frac{n!}{k!(n-k)!}$

# 2 Random Variables and Distribution

1. **ç´ç”Ÿ**:
   å‡¸: $E(g(X))\geq g(E(X))$
   å‡¹: $E(g(X))\leq g(E(X))$

- $E(|X|) \geq |E(X)|$ $(g(x) = |x|)$
- $E(X^2) \geq (E(X))^2$ $(g(x) = x^2)$
- $E(|X|^p) \geq |E(X)|^p$ å¯¹äº $p \geq 1$ $(g(x) = |x|^p,  p \geq 1)$
- $E(e^{cX}) \geq e^{cE(X)}$ $(g(x) = e^{cX})$

2. $Y=g(X)$, $h(y)=g^{-1}(y)$,
   $\Rightarrow f_Y(y)=\left\{\begin{aligned}|h'(y)|\cdot f_X(h(y)), åœ¨h(y)æœ‰å®šä¹‰å¤„\\
   0ï¼Œå¦åˆ™\end{aligned}\right.$
3. | åˆ†å¸ƒ                   | PDF/PMF                                                      | æœŸæœ›                | æ–¹å·®                  |
   | :--------------------- | ------------------------------------------------------------ | ------------------- | --------------------- |
   | å‡åŒ€ $(a, b)$          | $f(x) = \begin{cases} \frac{1}{b-a}, & \text{if } a < x < b \\ 0, & \text{otherwise} \end{cases}$ | $\frac{a+b}{2}$     | $\frac{(b-a)^2}{12}$  |
   | æ­£æ€ $(\mu, \sigma^2)$ | $f(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ | $\mu$               | $\sigma^2$            |
   | æŒ‡æ•° $(\lambda)$       | $f(x) = \begin{cases} \lambda e^{-\lambda x}, & \text{if } x \geq 0 \\ 0, & \text{otherwise} \end{cases}$ | $\frac{1}{\lambda}$ | $\frac{1}{\lambda^2}$ |
   | å‡ ä½• (p)(æ— è®°å¿†ç¦»æ•£)   | $p(x)=p(1-p)^{x-1}$                                          | $\frac{1}{p}$       | $\frac{1-p}{p^2}$     |
   | æ³Šæ¾($\lambda$)        | $p(x)=\frac{\lambda^x}{x!}e^{-\lambda}$                      | $\lambda$           | $\lambda$             |
   | äºŒé¡¹(n,p)              | $p(x)=C_n^xp^x(1-p)^{n-x}$                                   | np                  | np(1-p)               |
   | ä¼¯åŠªåˆ©(p)              | $p(x)=p^x(1-p)^{1-x}$                                        | p                   | p(1-p)                |
   
4. æ³Šæ¾: è®¡ç®—åœ¨ç‰¹å®šæ—¶é—´æ®µå†…å‘ç”Ÿ **k** æ¬¡äº‹ä»¶çš„æ¦‚ç‡;
   
   æŒ‡æ•°: è®¡ç®—ä¸‹ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿçš„æ—¶é—´æ˜¯å¦åœ¨æŸä¸ªæ—¶é—´ç‚¹ä¹‹å‰
5. æ ‡å‡†åŒ–ï¼š$Z=\frac{X-\mu}{\sigma}$
6. $E(X)=\int_{-\infty}^{\infty}xf(x)dx$
7. $E(Y)=E(g(X))=\int_{-\infty}^{\infty}g(x)f(x)dx$
8. $\text{Var(X)}=E(X-E(X))^2=E(X^2)-[E(X)]^2=\int_{-\infty}^{\infty}[x-E(X)]^2f(x)dx$

# 3 Joint Distributions

1. $\text{Cov}(X,Y)=E(XY)-E(X)E(Y)=E[(X-E(X))(Y-E(Y))]$

2. $\text{Cov}(aX,bY)=ab\text{Cov}(X,Y)$

3. $\text{Var}(X\pm Y )=\text{Var}(X)+\text{Var}(Y)\pm 2\text{Cov}(X,Y)$

4. $\text{Cov}(\sum_{i=1}^{n}a_iX_i,\sum_{j=1}^{m}b_jY_j)=\sum_{i=1}^{n}\sum_{i=1}^{m}a_ib_j\text{Cov}(X_i,Y_j)$

5. $\rho_{XY}=Cor(X,Y)=\frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}}$

6. **MSE**:
   $b_0=\frac{\text{Cov}(X,Y)}{\text{Var}(X)}$
   $a_0=E(Y)-\frac{\text{Cov}(X,Y)}{\text{Var}(X)}E(X)$
   $\min\limits_{a,b}MSE=\text{Var}(Y)(1-\rho^2_{XY})$

7. **Law of total expectation**: $E(X)=E((X|Y))$

   $E(X)=\int_{-\infty}^\infty E(X|Y=y)\cdot f_Y(y)dy$

8. **convolution (å·ç§¯)**: $ f_z(z)=f_X*f_Y\triangleq\int_{-\infty}^\infty f_X(z-y)f_Y(y)dx=\int_{-\infty}^\infty f_X(x)f_Y(z-x)dx$, (X,Yç‹¬ç«‹)

9. **ä¼½é©¬åˆ†å¸ƒ**$Gamma(r,\lambda)$: $ f_Z(z)= \begin{cases}\frac{\lambda^r}{(r-1)!}z^{r-1}e^{-\lambda z}, & z>0 \\ 0, & \text{otherwise} \end{cases}$, $Z=T_1+T_2+...+T_r$

10. **Central Limit Theorem (CLT, ä¸­å¿ƒæé™å®šç†)**: 
    $$
    Z_n=\frac{S_n-\mathrm{E}(S_n)}{\sqrt{\mathrm{Var}(S_n)}}
    =\frac{S_n-n\mu}{\sqrt{n\sigma^2}}
    =\frac{\sqrt{n}\,(\bar X_n-\mu)}{\sigma}.
    $$

â€‹	$Z_n$ **converges in distribution (ä¾åˆ†å¸ƒæ”¶æ•›)** :  
$$
F_{Z_n}(z)=\Pr(Z_n\le z)\xrightarrow[n\to\infty]{}\Phi(z)\;\text{for all }z.
$$

11. **normal approximation to binomial distribution (äºŒé¡¹åˆ†å¸ƒçš„æ­£æ€è¿‘ä¼¼)**:  
    $$
    % æ ‡å‡†åŒ–åè¿‘ä¼¼æ ‡å‡†æ­£æ€
    \frac{S_n-\mathrm{E}(S_n)}{\sqrt{\mathrm{Var}(S_n)}}
    =\frac{S_n-np}{\sqrt{np(1-p)}}
    \ \overset{\text{approx.}}{\sim}\ \mathcal{N}(0,1)\\
    S_n\ \overset{\text{approx.}}{\sim}\ \mathcal{N}\!\big(np,\;np(1-p)\big)
    $$

12. When $p$ is small, the Poisson approximation is better while the normal approximation is better for large $p$.

13. **bivariate normal distribution (äºŒå…ƒæ­£æ€åˆ†å¸ƒ)**ï¼š
    $$
    f(x,y)
    = \frac{1}{2\pi\,\sigma_X\sigma_Y\sqrt{1-\rho^2}}
    \exp\!\left\{
    -\frac{1}{2(1-\rho^2)}
    \left[
    \frac{(x-\mu_X)^2}{\sigma_X^2}
    - 2\rho\,\frac{(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y}
    + \frac{(y-\mu_Y)^2}{\sigma_Y^2}
    \right]
    \right\}.
    $$

    $$
    \begin{pmatrix}X\\ Y\end{pmatrix}
    \sim \mathcal{N}\!\left(
    \begin{pmatrix}\mu_X\\ \mu_Y\end{pmatrix},
    \begin{pmatrix}
    \sigma_X^2 & \rho\,\sigma_X\sigma_Y\\
    \rho\,\sigma_X\sigma_Y & \sigma_Y^2
    \end{pmatrix}
    \right)
    = \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma}).
    $$

    

14. X and Y are independent $\to$ X and Y are uncorrelated

15. $$
    X|Y=y\sim N(\mu_X+\frac{\rho\sigma_X}{\sigma_Y}(y-\mu_Y),(1-\rho^2)\sigma_X^2)
    $$

# 4 Monte Carlo Methods

1.  **weak law of large numbers (LLN, å¤§æ•°å®šå¾‹)**

   - $X_1, X_2, \ldots$ random variables  
     
     $\mu \triangleq E(X_i) < \infty$,  
     $\bar X_n = (X_1 + X_2 + \cdots + X_n)/n$.
     
   - $\bar X_n$ **converges in probability**ï¼ˆä¾æ¦‚ç‡æ”¶æ•›ï¼‰to $\mu$ as $n \to \infty$:

     $$\bar X_n \xrightarrow{P} \mu,\quad \text{as } n \to \infty.$$

   - For any $\varepsilon > 0$,

     $$\lim_{n\to\infty} P\left(\left|\frac{1}{n}\sum_{i=1}^n X_i - \mu\right| > \varepsilon\right) = 0.$$
     
   - $\bar X_n\overset{\text{approx.}}{\sim}N(\mu,\frac{\sigma^2}{n})$

2.  **Inverse transformation sampling (é€†å˜æ¢é‡‡æ ·)**

   è‹¥F(x)æ˜¯æŸä¸ªè¿ç»­éšæœºå˜é‡çš„åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ï¼Œä¸”å…¶åå‡½æ•° $F^{-1}(x)$å­˜åœ¨ï¼Œ
    ä»¤$U \sim U[0,1]$ï¼Œåˆ™$X = F^{-1}(U) \sim F(x)$.

3. $N(0,1)$åˆ†å¸ƒçš„éšæœºæ•°: 

   è‹¥ $U_1 \sim U[0,1]$ï¼Œ$U_2 \sim U[0,1]$ç‹¬ç«‹ï¼Œä»¤

$$
Z_1 = \sqrt{-2\ln U_1}\cos(2\pi U_2),\quad   Z_2 = \sqrt{-2\ln U_1}\sin(2\pi U_2),
$$

åˆ™ $Z_1$, $Z_2$ä¸ºä¸€å¯¹ç‹¬ç«‹çš„æ ‡å‡†æ­£æ€éšæœºå˜é‡ã€‚

å¯¹äº$N(\mu,\sigma^2)$ï¼Œ $y_i=\mu+\sigma z_i$.

4. **æ‹’ç»é‡‡æ ·**

   ç›®æ ‡åˆ†å¸ƒï¼ˆtarget distribution) ä»æŸä¸ªéš¾ä»¥ç›´æ¥é‡‡æ ·çš„åˆ†å¸ƒ, å…¶ PDF ä¸º f(x)ï¼Œé‡‡æ ·ã€‚

   å‚è€ƒåˆ†å¸ƒ/å»ºè®®åˆ†å¸ƒï¼ˆproposal distributionï¼‰æ”¹ä¸ºä»ä¸€ä¸ªå®¹æ˜“é‡‡æ ·çš„åˆ†å¸ƒ g(x) å–æ ·

   - ä»proposalåˆ†å¸ƒg(x)é‡‡æ ·å¾—åˆ°$x_i$

   - $u_i\sim U[0,1]$

   - æ‹’ç»$x_i$è‹¥$u_i>\frac{f(x_i)}{cg(x_i)}$

   - æ¥å— 
     $$
     P\!\left(U \le \frac{f(X)}{c\,g(X)}\right)
     = \int_{-\infty}^{\infty} P\!\left(U \le \frac{f(X)}{c\,g(X)} \,\middle|\, X = x\right) g(x)\,dx
     = \int_{-\infty}^{\infty} \frac{f(x)}{c\,g(x)} g(x)\,dx
     = \frac{1}{c}.
     $$

   - ä¸ºæé«˜æ•ˆç‡, $c$å°½é‡å°

   - $c=\sup_x\frac{f(x)}{g(x)}$,ç†æƒ³æƒ…å†µä¸‹æ¥å—ç‡æœ€é«˜   

5. CDF of the accepted numbers $F(t)=\int_{-\infty}^t f(x)dx$

6. 

   

# 5 Basic Concepts in Statistics

> $X_1,...,X_n$ æ˜¯æ¥è‡ªæ€»ä½“$ Xâˆ¼f(x;\theta_1,...,\theta_k),Xâˆ¼f(x;\theta_1,...,\theta_k)$ çš„ä¸€ä¸ªç®€å•éšæœºæ ·æœ¬

1. Sample variance (æ ·æœ¬æ–¹å·®)
   $$
   S^2=\frac1{n-1}\sum_{i=1}^n(X_i-\bar{X})^2
   $$
   

2. **Sample p-quantile** (æ ·æœ¬påˆ†ä½æ•°)
   $$
   Q_p=\begin{cases} 
   X_{(np+1)}, & \text{å¦‚æœ } np \text{ ä¸æ˜¯æ•´æ•°} \\ 
   \frac{X_{(np)} + X_{(np+1)}}{2}, & \text{å¦‚æœ } np \text{ æ˜¯æ•´æ•°}
   \end{cases}
   $$
   

3. è‹¥$\forall\theta\in\Theta$, $E_\theta(\hat\theta_l)=\theta_l$, åˆ™ $\hat\theta_l$ æ˜¯$\theta$çš„**æ— åä¼°è®¡é‡**, å¦åˆ™æ˜¯**æœ‰åä¼°è®¡é‡**. $E_\theta(\hat\theta_l)-\theta_l$æ˜¯åå·®. è‹¥åå·®ä¸ä¸º0ä½†æ”¶æ•›ä¸º0($n\to\infty$), åˆ™ $\hat\theta_l$æ˜¯ $\theta$ **æ¸è¿‘æ— åä¼°è®¡é‡**

4. å¦‚æœå¯¹äº âˆ€ğœƒâˆˆÎ˜å’Œ âˆ€ğœ–>0ï¼Œæˆ‘ä»¬æœ‰
   $$
   \lim_{n\to\infty}P_\theta(|\hat\theta_i-\theta_i|>\epsilon)=0
   $$
   

   é‚£ä¹ˆ$\hat\theta_i$ç§°ä¸º$\theta_i$çš„ä¸€ä¸ª**ç›¸åˆä¼°è®¡é‡**ã€‚

5. 

- ä¸€ä¸ªï¼ˆæ¸è¿‘ï¼‰æ— åä¼°è®¡é‡å¯èƒ½ä¸æ˜¯ä¸€ä¸ªç›¸åˆä¼°è®¡é‡ã€‚
- ä¸€ä¸ªç›¸åˆä¼°è®¡é‡å¯èƒ½ä¸æ˜¯ä¸€ä¸ªï¼ˆæ¸è¿‘ï¼‰æ— åä¼°è®¡é‡ã€‚

6. å¦‚æœ$\hat\theta$ æ˜¯$\theta$çš„ä¸€ä¸ªæ¸è¿‘æ— åä¼°è®¡é‡ï¼Œå¹¶ä¸”å½“$n\to\infty$ æ—¶$Var(\hat\theta)\to0$ï¼Œé‚£ä¹ˆ$\hat\theta$æ˜¯$\theta$çš„ä¸€ä¸ªç›¸åˆä¼°è®¡é‡

7. **åˆ‡æ¯”é›ªå¤«ä¸ç­‰å¼**

   Let X be a random variable with mean ğœ‡ = E(X) and variance $\sigma ^2$ = Var(X) both exists, then
   $$
   P(|X-\mu|\ge k\sigma)\le\frac1{k^2}
   $$

8. No matter what is the population distribution, if the population mean $\mu=E(X)$ and population
   variance $\sigma=Var(X)$ exist, then $\hat\mu=\bar X$ and $\hat\sigma^2=S$ are consistent estimators of $\mu$ and $\sigma ^2$  

9.  **æ€»ä½“ğ‘˜é˜¶çŸ©**: $\mu_k=E(X^k)$    

   **æ ·æœ¬ğ‘˜é˜¶çŸ©**: $M_k=\frac1n\sum_{i=1}^nX^k_i$  

   **æ€»ä½“ğ‘˜é˜¶ä¸­å¿ƒçŸ©**: $\tilde\mu_k=E[(X-\mu_1)^k]$    

   **æ ·æœ¬ğ‘˜é˜¶ä¸­å¿ƒçŸ©**: $\tilde M_k=\frac1n\sum_{i=1}^n(X_i-\bar X)^k$  

10. ä¸€ä¸ªå‚æ•°çš„çŸ©ä¼°è®¡é‡å¯èƒ½ä¸æ˜¯å”¯ä¸€çš„ã€‚

11. **ä¼¼ç„¶å‡½æ•°**: 
    $$
    L(\theta;x_1,
     ,...,x_n)=\Pi_{i=1}^nf(x_i;\theta),\theta\in\Theta.
    $$
    å¦‚æœå­˜åœ¨ $\hat{\theta} = \hat{\theta}(x_1, ..., x_n)$ ä½¿å¾—

    $$
    L(\hat{\theta}) = \max_{\theta \in \Theta} L(\theta; x_1, ..., x_n),
    $$

    (ä¼°è®¡é‡å’Œä¼°è®¡å€¼éƒ½å¯ä»¥è¡¨ç¤ºä¸º $\hat{\theta}_0$ã€‚)

    - é‚£ä¹ˆ $\hat{\theta}(x_1, ..., x_n)$ æ˜¯ $\theta$ çš„æå¤§ä¼¼ç„¶ä¼°è®¡å€¼ (æå¤§ä¼¼ç„¶ä¼°è®¡å€¼)ï¼Œå¯¹åº”çš„ä¼°è®¡é‡ $\hat{\theta}(X_1, ..., X_n)$ æ˜¯ $\theta$ çš„æå¤§ä¼¼ç„¶ä¼°è®¡é‡ (æå¤§ä¼¼ç„¶ä¼°è®¡é‡)ã€‚

    - æœ€å¤§åŒ– $L(\theta; x)$ ç­‰ä»·äºæœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•° (å¯¹æ•°ä¼¼ç„¶å‡½æ•°):

    $$
    \ell(\theta; x) = \log L(\theta; x) = \sum_{i=1}^n \log f(x_i; \theta).
    $$

12. - åœ¨æ¸©å’Œçš„æ­£åˆ™æ¡ä»¶ä¸‹ï¼ŒçŸ©ä¼°è®¡é‡å’Œæå¤§ä¼¼ç„¶ä¼°è®¡é‡éƒ½æ˜¯ç›¸åˆä¸”æ¸è¿‘æ— åçš„ä¼°è®¡é‡ã€‚
    - åœ¨æ¸©å’Œçš„æ­£åˆ™æ¡ä»¶ä¸‹ï¼Œå¯¹äºå¤§æ ·æœ¬ï¼Œæå¤§ä¼¼ç„¶ä¼°è®¡é‡å…·æœ‰è¿‘ä¼¼æ­£æ€åˆ†å¸ƒã€‚è¿™ä¸ªæ€§è´¨ç§°ä¸º**æ¸è¿‘æ­£æ€æ€§** (**æ¸è¿‘æ­£æ€æ€§**)ã€‚

13. | åˆ†å¸ƒ              | ä¼¼ç„¶å‡½æ•°                                                     | å¯¹æ•°ä¼¼ç„¶å‡½æ•°                                                 | æå¤§ä¼¼ç„¶ä¼°è®¡é‡                                               |
    | ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | $U(0,\theta)$     | $\frac1{\theta^n}$                                           | $-n\log n$                                                   | $X_{(n)}$                                                    |
    | $N(\mu,\sigma^2)$ | $(\sqrt{2\pi})^{-n} (\sigma^2)^{\frac{n}{2}} e^{-\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2}$ | $-n \log (\sqrt{2\pi}) - \frac{n}{2} \log (\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2$ | $\hat\mu=\bar{X}, \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 = \hat{S}^2$ |

    

14. å¯¹äº $\forall \alpha \in (0, 1)$ï¼Œå¦‚æœå­˜åœ¨ä¸¤ä¸ªç»Ÿè®¡é‡ $\hat{\theta}_{11}(X_1, ..., X_n)$ å’Œ $\hat{\theta}_{12}(X_1, ..., X_n)$ ä½¿å¾—  

    $$
    P_\Theta(\hat{\theta}_{11} < \theta_i < \hat{\theta}_{12}) \geq 1 - \alpha, \forall \Theta \in \Theta,
    $$
     é‚£ä¹ˆ $(\hat{\theta}_{i1}, \hat{\theta}_{i2})$ ç§°ä¸º $\theta_i$ çš„ä¸€ä¸ªç½®ä¿¡æ°´å¹³ä¸º $1 - \alpha$ çš„**ç½®ä¿¡åŒºé—´**ï¼ˆç½®ä¿¡æ°´å¹³ä¸º $1 - \alpha$ çš„ç½®ä¿¡åŒºé—´ï¼‰ï¼Œæˆ–ç®€ç§°ä¸ºä¸€ä¸ª $100(1 - \alpha)\%$ ç½®ä¿¡åŒºé—´ã€‚

15. $X\sim N(\mu,\sigma ^2)$, $P(\bar X-c_1<\mu<\bar X+c_2)=1-\alpha$, $\bar X\sim   N(\mu, \frac{\sigma^2}n)$  

    $c_1=c_2=z_{\alpha/2}\frac\sigma{\sqrt{n}}$,  $z_{\alpha/2}$æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ä¸Š$\alpha/2$åˆ†ä½ç‚¹ã€‚

16. $X\sim N(\mu,\sigma ^2)$, å¦‚æœ$\sigma ^2$æœªçŸ¥ï¼Œæ±‚$\mu$çš„ç½®ä¿¡åŒºé—´. 
    $$
    1-\alpha=P(\mu-c_2<\bar X<\mu+c_1)=P(-\frac{\sqrt nc_2}S<T<\frac{\sqrt nc_1}S)\\
    T=\frac{\bar X-\mu}{S/\sqrt n}
    $$
    å½“nå¾ˆå¤§æ—¶ï¼Œ$T\overset{approx}\sim N(0,1)$

    **å¤§æ ·æœ¬ç½®ä¿¡åŒºé—´**: $( \overline{X} - z_{\alpha/2} \frac{S}{\sqrt{n}}, \overline{X} + z_{\alpha/2} \frac{S}{\sqrt{n}}).$  

17. - $\hat{\theta}_i(X_1, \ldots, X_n)$ æ˜¯ $\theta_i$ çš„ä¸€ä¸ªæ— åä¼°è®¡é‡ï¼Œä¸” $\hat{\theta}_i$ çš„æ ‡å‡†å·®ä¸º $\sigma(\hat{\theta}_i) = SD(\hat{\theta}_i)$ï¼ˆç§°ä¸º $\hat{\theta}_i$ çš„æ ‡å‡†è¯¯å·®ï¼ˆæ ‡å‡†è¯¯å·®ï¼‰ï¼‰ã€‚

    - å¦‚æœ $\hat{\theta}_i$ ç²¾ç¡®æœä»æ­£æ€åˆ†å¸ƒï¼Œå³ $\hat{\theta}_i \sim N(\theta_i, \sigma^2(\hat{\theta}_i))$ï¼Œå¹¶ä¸” $\sigma^2(\hat{\theta}_i)$ ä¸ä¾èµ–äºä»»ä½•æœªçŸ¥å‚æ•°ï¼Œé‚£ä¹ˆ $\theta_i$ çš„ä¸€ä¸ªç²¾ç¡® $100(1 - \alpha)\%$ ç½®ä¿¡åŒºé—´ä¸º
    $$
    \hat{\theta}_i \pm z_{\alpha/2}\sigma(\hat{\theta}_i) = (\hat{\theta}_i - z_{\alpha/2}\sigma(\hat{\theta}_i), \hat{\theta}_i + z_{\alpha/2}\sigma(\hat{\theta}_i)).
    $$

    - å¦‚æœ $\hat{\theta}_i$ çš„ $N(\theta_i, \sigma^2(\hat{\theta}_i))$ æˆ– $\sigma^2(\hat{\theta}_i)$ ä¾èµ–äºæœªçŸ¥å‚æ•°ä¸” $\sigma^2(\hat{\theta}_i)$ æ˜¯ $\sigma^2(\hat{\theta}_i)$ çš„ä¸€ä¸ªç›¸åˆä¼°è®¡é‡ï¼Œé‚£ä¹ˆ $\theta_i$ çš„ä¸€ä¸ªå¤§æ ·æœ¬ $100(1 - \alpha)\%$ ç½®ä¿¡åŒºé—´ä¸º
    $$
    \hat{\theta}_i \pm z_{\alpha/2}\hat{\sigma}(\hat{\theta}_i) = (\hat{\theta}_i - z_{\alpha/2}\hat{\sigma}(\hat{\theta}_i), \hat{\theta}_i + z_{\alpha/2}\hat{\sigma}(\hat{\theta}_i)).
    $$

    - å¸¸ç”¨å€¼ï¼š$z_{0.10} = 1.282$, $z_{0.05} = 1.645$, $z_{0.025} = 1.960$, $z_{0.01} = 2.326$, $z_{0.005} = 2.576$.

# 6

1. 
